{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a6e802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM 6: RIDGE REGRESSION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART 1: CLOSED-FORM SOLUTION DERIVATION\n",
      "======================================================================\n",
      "\n",
      "Ridge Regression Loss Function:\n",
      "  J(theta) = sum[(y_pred - y)^2] + lambda * sum[theta_j^2]\n",
      "\n",
      "Matrix form:\n",
      "  J(theta) = (X*theta - y)^T * (X*theta - y) + lambda * theta^T * theta\n",
      "\n",
      "Expand:\n",
      "  J(theta) = theta^T*X^T*X*theta - 2*theta^T*X^T*y + y^T*y + lambda*theta^T*theta\n",
      "\n",
      "Take gradient with respect to theta:\n",
      "  dJ/d(theta) = 2*X^T*X*theta - 2*X^T*y + 2*lambda*theta\n",
      "\n",
      "Set gradient to zero:\n",
      "  2*X^T*X*theta - 2*X^T*y + 2*lambda*theta = 0\n",
      "  X^T*X*theta + lambda*theta = X^T*y\n",
      "  (X^T*X + lambda*I)*theta = X^T*y\n",
      "\n",
      "CLOSED-FORM SOLUTION:\n",
      "  theta = (X^T*X + lambda*I)^(-1) * X^T * y\n",
      "\n",
      "Where I is the identity matrix of size (n_features x n_features).\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PART 2: GRADIENT DESCENT IMPLEMENTATION\n",
      "======================================================================\n",
      "\n",
      "Implemented gradient_descent_ridge() function\n",
      "Key features:\n",
      "  - Adds regularization term to gradient: 2*lambda*theta\n",
      "  - Does NOT regularize intercept (theta[0])\n",
      "  - Uses learning rate alpha and runs for num_iterations\n",
      "\n",
      "======================================================================\n",
      "PART 3: SIMULATED DATA EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "Simulated Data:\n",
      "  Sample size: N = 1000\n",
      "  True relationship: Y = 1 + 2*X + noise\n",
      "  True intercept: 1\n",
      "  True slope: 2\n",
      "  Noise: N(0, sqrt(2))\n",
      "\n",
      "Data statistics:\n",
      "  X range: [-1.98, 2.00]\n",
      "  Y range: [-5.90, 8.23]\n",
      "  Y mean: 1.06\n",
      "  Feature matrix shape: (1000, 2)\n",
      "\n",
      "======================================================================\n",
      "PART 4: RIDGE REGRESSION WITH DIFFERENT LAMBDA VALUES\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 0\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.1377  (true = 1.0, error = 0.1377)\n",
      "  Slope:     1.9453  (true = 2.0, error = 0.0547)\n",
      "  MSE:       1.9499\n",
      "  R^2:       0.7258\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 1\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.1377  (true = 1.0, error = 0.1377)\n",
      "  Slope:     1.9439  (true = 2.0, error = 0.0561)\n",
      "  MSE:       1.9499\n",
      "  R^2:       0.7258\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 10\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.1372  (true = 1.0, error = 0.1372)\n",
      "  Slope:     1.9311  (true = 2.0, error = 0.0689)\n",
      "  MSE:       1.9502\n",
      "  R^2:       0.7258\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 100\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.1325  (true = 1.0, error = 0.1325)\n",
      "  Slope:     1.8124  (true = 2.0, error = 0.1876)\n",
      "  MSE:       1.9740\n",
      "  R^2:       0.7224\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 1000\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.1057  (true = 1.0, error = 0.1057)\n",
      "  Slope:     1.1225  (true = 2.0, error = 0.8775)\n",
      "  MSE:       2.8735\n",
      "  R^2:       0.5960\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Lambda = 10000\n",
      "----------------------------------------------------------------------\n",
      "  Intercept: 1.0710  (true = 1.0, error = 0.0710)\n",
      "  Slope:     0.2335  (true = 2.0, error = 1.7665)\n",
      "  MSE:       5.9471\n",
      "  R^2:       0.1638\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      " Lambda  Intercept    Slope      MSE       R2\n",
      "      0   1.137727 1.945275 1.949936 0.725824\n",
      "      1   1.137671 1.943850 1.949939 0.725823\n",
      "     10   1.137175 1.931119 1.950209 0.725785\n",
      "    100   1.132549 1.812414 1.974016 0.722438\n",
      "   1000   1.105658 1.122450 2.873516 0.595961\n",
      "  10000   1.071013 0.233509 5.947067 0.163796\n",
      "\n",
      "======================================================================\n",
      "TRUE VALUES:\n",
      "  Intercept = 1.0\n",
      "  Slope = 2.0\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. EFFECT OF LAMBDA ON COEFFICIENTS:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   Lambda =     0: Slope = 1.9453 (no regularization)\n",
      "   Lambda =     1: Slope = 1.9439 (97.2% of true value)\n",
      "   Lambda =    10: Slope = 1.9311 (96.6% of true value)\n",
      "   Lambda =   100: Slope = 1.8124 (90.6% of true value) [heavy shrinkage]\n",
      "   Lambda =  1000: Slope = 1.1225 (56.1% of true value) [heavy shrinkage]\n",
      "   Lambda = 10000: Slope = 0.2335 (11.7% of true value) [heavy shrinkage]\n",
      "\n",
      "2. EFFECT OF LAMBDA ON MODEL PERFORMANCE:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   Best MSE: Lambda = 0 (MSE = 1.9499)\n",
      "   Best R^2: Lambda = 0 (R^2 = 0.7258)\n",
      "\n",
      "   As lambda increases:\n",
      "   - Coefficients shrink toward zero\n",
      "   - MSE increases (worse fit to training data)\n",
      "   - R^2 decreases (less variance explained)\n",
      "\n",
      "3. BIAS-VARIANCE TRADEOFF:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   Lambda = 0 (No regularization):\n",
      "     Most accurate coefficients: Slope = 1.9453\n",
      "     Best fit: MSE = 1.9499\n",
      "     High variance, low bias\n",
      "\n",
      "   Lambda = 10000 (Heavy regularization):\n",
      "     Highly shrunk coefficients: Slope = 0.2335\n",
      "     Worse fit: MSE = 5.9471\n",
      "     Low variance, high bias\n",
      "\n",
      "   Regularization trades training accuracy for model simplicity\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROBLEM 6: RIDGE REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    return X @ theta\n",
    "\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared score\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "\n",
    "def gradient_descent_ridge(X, y, alpha, num_iterations, lambda_reg):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for ridge regression\n",
    "    \n",
    "    Cost function: J(theta) = (1/n)*sum[(X*theta - y)^2] + lambda*sum[theta_j^2]\n",
    "    \n",
    "    Gradient: dJ/d(theta) = (2/n)*X^T*(X*theta - y) + 2*lambda*theta\n",
    "    \n",
    "    Note: We typically DON'T regularize the intercept (theta[0])\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array (n_samples, n_features)\n",
    "        Feature matrix with intercept column\n",
    "    y : array (n_samples,)\n",
    "        Target values\n",
    "    alpha : float\n",
    "        Learning rate\n",
    "    num_iterations : int\n",
    "        Number of iterations\n",
    "    lambda_reg : float\n",
    "        Regularization parameter (lambda)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    theta : array (n_features,)\n",
    "        Optimized parameters\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    theta = np.zeros(n_features)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Predictions\n",
    "        predictions = X @ theta\n",
    "        \n",
    "        # Errors\n",
    "        errors = predictions - y\n",
    "        \n",
    "        # Gradient of MSE term: (2/n) * X^T @ errors\n",
    "        gradient = (2 / n_samples) * X.T @ errors\n",
    "        \n",
    "        # Add regularization gradient for all coefficients EXCEPT intercept\n",
    "        # theta[0] is intercept, theta[1:] are feature coefficients\n",
    "        gradient[1:] += (2 * lambda_reg / n_samples) * theta[1:]\n",
    "        \n",
    "        # Update theta\n",
    "        theta = theta - alpha * gradient\n",
    "    \n",
    "    return theta\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 1: ANALYTICAL SOLUTION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 1: CLOSED-FORM SOLUTION DERIVATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "Ridge Regression Loss Function:\n",
    "  J(theta) = sum[(y_pred - y)^2] + lambda * sum[theta_j^2]\n",
    "\n",
    "Matrix form:\n",
    "  J(theta) = (X*theta - y)^T * (X*theta - y) + lambda * theta^T * theta\n",
    "\n",
    "Expand:\n",
    "  J(theta) = theta^T*X^T*X*theta - 2*theta^T*X^T*y + y^T*y + lambda*theta^T*theta\n",
    "\n",
    "Take gradient with respect to theta:\n",
    "  dJ/d(theta) = 2*X^T*X*theta - 2*X^T*y + 2*lambda*theta\n",
    "\n",
    "Set gradient to zero:\n",
    "  2*X^T*X*theta - 2*X^T*y + 2*lambda*theta = 0\n",
    "  X^T*X*theta + lambda*theta = X^T*y\n",
    "  (X^T*X + lambda*I)*theta = X^T*y\n",
    "\n",
    "CLOSED-FORM SOLUTION:\n",
    "  theta = (X^T*X + lambda*I)^(-1) * X^T * y\n",
    "\n",
    "Where I is the identity matrix of size (n_features x n_features).\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 2: IMPLEMENT RIDGE REGRESSION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 2: GRADIENT DESCENT IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nImplemented gradient_descent_ridge() function\")\n",
    "print(\"Key features:\")\n",
    "print(\"  - Adds regularization term to gradient: 2*lambda*theta\")\n",
    "print(\"  - Does NOT regularize intercept (theta[0])\")\n",
    "print(\"  - Uses learning rate alpha and runs for num_iterations\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 3: SIMULATE DATA\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 3: SIMULATED DATA EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data: Y = 1 + 2*X + noise\n",
    "N = 1000\n",
    "X_sim = np.random.uniform(-2, 2, N)\n",
    "noise = np.random.normal(0, np.sqrt(2), N)  # noise ~ N(0, 2)\n",
    "Y_sim = 1 + 2 * X_sim + noise\n",
    "\n",
    "print(f\"\\nSimulated Data:\")\n",
    "print(f\"  Sample size: N = {N}\")\n",
    "print(f\"  True relationship: Y = 1 + 2*X + noise\")\n",
    "print(f\"  True intercept: 1\")\n",
    "print(f\"  True slope: 2\")\n",
    "print(f\"  Noise: N(0, sqrt(2))\")\n",
    "print(f\"\\nData statistics:\")\n",
    "print(f\"  X range: [{X_sim.min():.2f}, {X_sim.max():.2f}]\")\n",
    "print(f\"  Y range: [{Y_sim.min():.2f}, {Y_sim.max():.2f}]\")\n",
    "print(f\"  Y mean: {Y_sim.mean():.2f}\")\n",
    "\n",
    "# Prepare data with intercept\n",
    "X_sim_with_int = np.c_[np.ones(N), X_sim]\n",
    "\n",
    "print(f\"  Feature matrix shape: {X_sim_with_int.shape}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# PART 4: TEST DIFFERENT LAMBDA VALUES\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PART 4: RIDGE REGRESSION WITH DIFFERENT LAMBDA VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Lambda values to test\n",
    "lambdas = [0, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "# Hyperparameters for gradient descent\n",
    "alpha = 0.01\n",
    "num_iterations = 2000\n",
    "\n",
    "results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Lambda = {lam}\")\n",
    "    print('-'*70)\n",
    "    \n",
    "    # Train ridge regression\n",
    "    theta_ridge = gradient_descent_ridge(\n",
    "        X_sim_with_int, \n",
    "        Y_sim, \n",
    "        alpha=alpha, \n",
    "        num_iterations=num_iterations, \n",
    "        lambda_reg=lam\n",
    "    )\n",
    "    \n",
    "    # Extract coefficients\n",
    "    intercept = theta_ridge[0]\n",
    "    slope = theta_ridge[1]\n",
    "    \n",
    "    # Make predictions\n",
    "    Y_pred = predict(X_sim_with_int, theta_ridge)\n",
    "    \n",
    "    # Compute metrics\n",
    "    mse = compute_mse(Y_sim, Y_pred)\n",
    "    r2 = compute_r2(Y_sim, Y_pred)\n",
    "    \n",
    "    # Compare with true values\n",
    "    intercept_error = abs(intercept - 1.0)\n",
    "    slope_error = abs(slope - 2.0)\n",
    "    \n",
    "    print(f\"  Intercept: {intercept:.4f}  (true = 1.0, error = {intercept_error:.4f})\")\n",
    "    print(f\"  Slope:     {slope:.4f}  (true = 2.0, error = {slope_error:.4f})\")\n",
    "    print(f\"  MSE:       {mse:.4f}\")\n",
    "    print(f\"  R^2:       {r2:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Lambda': lam,\n",
    "        'Intercept': intercept,\n",
    "        'Slope': slope,\n",
    "        'MSE': mse,\n",
    "        'R2': r2,\n",
    "        'Intercept_Error': intercept_error,\n",
    "        'Slope_Error': slope_error\n",
    "    })\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display_df = results_df[['Lambda', 'Intercept', 'Slope', 'MSE', 'R2']]\n",
    "print(\"\\n\" + display_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRUE VALUES:\")\n",
    "print(\"  Intercept = 1.0\")\n",
    "print(\"  Slope = 2.0\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. EFFECT OF LAMBDA ON COEFFICIENTS:\")\n",
    "print(\"   \" + \"-\"*66)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    lam = int(row['Lambda'])\n",
    "    slope = row['Slope']\n",
    "    slope_pct = (slope / 2.0) * 100  # Percentage of true slope\n",
    "    \n",
    "    if lam == 0:\n",
    "        print(f\"\\n   Lambda = {lam:5d}: Slope = {slope:.4f} (no regularization)\")\n",
    "    elif lam < 100:\n",
    "        print(f\"   Lambda = {lam:5d}: Slope = {slope:.4f} ({slope_pct:.1f}% of true value)\")\n",
    "    else:\n",
    "        print(f\"   Lambda = {lam:5d}: Slope = {slope:.4f} ({slope_pct:.1f}% of true value) [heavy shrinkage]\")\n",
    "\n",
    "print(\"\\n2. EFFECT OF LAMBDA ON MODEL PERFORMANCE:\")\n",
    "print(\"   \" + \"-\"*66)\n",
    "\n",
    "best_mse_idx = results_df['MSE'].idxmin()\n",
    "best_r2_idx = results_df['R2'].idxmax()\n",
    "\n",
    "print(f\"\\n   Best MSE: Lambda = {int(results_df.loc[best_mse_idx, 'Lambda'])} (MSE = {results_df.loc[best_mse_idx, 'MSE']:.4f})\")\n",
    "print(f\"   Best R^2: Lambda = {int(results_df.loc[best_r2_idx, 'Lambda'])} (R^2 = {results_df.loc[best_r2_idx, 'R2']:.4f})\")\n",
    "\n",
    "print(\"\\n   As lambda increases:\")\n",
    "print(\"   - Coefficients shrink toward zero\")\n",
    "print(\"   - MSE increases (worse fit to training data)\")\n",
    "print(\"   - R^2 decreases (less variance explained)\")\n",
    "\n",
    "print(\"\\n3. BIAS-VARIANCE TRADEOFF:\")\n",
    "print(\"   \" + \"-\"*66)\n",
    "\n",
    "print(f\"\\n   Lambda = 0 (No regularization):\")\n",
    "print(f\"     Most accurate coefficients: Slope = {results_df.loc[0, 'Slope']:.4f}\")\n",
    "print(f\"     Best fit: MSE = {results_df.loc[0, 'MSE']:.4f}\")\n",
    "print(f\"     High variance, low bias\")\n",
    "\n",
    "high_lambda_idx = results_df['Lambda'].idxmax()\n",
    "print(f\"\\n   Lambda = {int(results_df.loc[high_lambda_idx, 'Lambda'])} (Heavy regularization):\")\n",
    "print(f\"     Highly shrunk coefficients: Slope = {results_df.loc[high_lambda_idx, 'Slope']:.4f}\")\n",
    "print(f\"     Worse fit: MSE = {results_df.loc[high_lambda_idx, 'MSE']:.4f}\")\n",
    "print(f\"     Low variance, high bias\")\n",
    "\n",
    "print(\"\\n   Regularization trades training accuracy for model simplicity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
