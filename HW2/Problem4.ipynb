{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14a8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM 4: POLYNOMIAL REGRESSION\n",
      "======================================================================\n",
      "\n",
      "STEP 1: Extract sqft_living feature\n",
      "----------------------------------------------------------------------\n",
      "Train samples: 1000\n",
      "Test samples:  1000\n",
      "sqft_living range: [380, 6070]\n",
      "\n",
      "STEP 2: Standardize sqft_living\n",
      "----------------------------------------------------------------------\n",
      "Original - Mean: 2051.20, Std: 887.93\n",
      "Scaled   - Mean: 0.000000, Std: 1.000000\n",
      "\n",
      "STEP 3: Train polynomial models for degrees 1-5\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "DEGREE p = 1\n",
      "======================================================================\n",
      "Polynomial feature matrix shape: (1000, 1)\n",
      "Features: X^1\n",
      "With intercept: (1000, 2)\n",
      "\n",
      "Coefficients:\n",
      "  θ₀ (intercept) = 520.4148\n",
      "  θ_1 (X^1)       = 239.2632\n",
      "\n",
      "Performance Metrics:\n",
      "  Training - MSE:    57,947.53  |  R²:  0.4967\n",
      "  Testing  - MSE:    88,575.98  |  R²:  0.4687\n",
      "\n",
      "======================================================================\n",
      "DEGREE p = 2\n",
      "======================================================================\n",
      "Polynomial feature matrix shape: (1000, 2)\n",
      "Features: X^1 X^2\n",
      "With intercept: (1000, 3)\n",
      "\n",
      "Coefficients:\n",
      "  θ₀ (intercept) = 484.7817\n",
      "  θ_1 (X^1)       = 194.8233\n",
      "  θ_2 (X^2)       = 35.6688\n",
      "\n",
      "Performance Metrics:\n",
      "  Training - MSE:    54,822.67  |  R²:  0.5238\n",
      "  Testing  - MSE:    71,791.68  |  R²:  0.5694\n",
      "\n",
      "======================================================================\n",
      "DEGREE p = 3\n",
      "======================================================================\n",
      "Polynomial feature matrix shape: (1000, 3)\n",
      "Features: X^1 X^2 X^3\n",
      "With intercept: (1000, 4)\n",
      "\n",
      "Coefficients:\n",
      "  θ₀ (intercept) = 463.8940\n",
      "  θ_1 (X^1)       = 211.4578\n",
      "  θ_2 (X^2)       = 71.9583\n",
      "  θ_3 (X^3)       = -12.3452\n",
      "\n",
      "Performance Metrics:\n",
      "  Training - MSE:    53,785.19  |  R²:  0.5329\n",
      "  Testing  - MSE:    99,833.48  |  R²:  0.4012\n",
      "\n",
      "======================================================================\n",
      "DEGREE p = 4\n",
      "======================================================================\n",
      "Polynomial feature matrix shape: (1000, 4)\n",
      "Features: X^1 X^2 X^3 X^4\n",
      "With intercept: (1000, 5)\n",
      "\n",
      "Coefficients:\n",
      "  θ₀ (intercept) = 464.8908\n",
      "  θ_1 (X^1)       = 159.0030\n",
      "  θ_2 (X^2)       = 65.3669\n",
      "  θ_3 (X^3)       = 20.4069\n",
      "  ... (and 1 more coefficients)\n",
      "\n",
      "Performance Metrics:\n",
      "  Training - MSE:    52,795.77  |  R²:  0.5415\n",
      "  Testing  - MSE:   250,979.27  |  R²: -0.5053\n",
      "\n",
      "======================================================================\n",
      "DEGREE p = 5\n",
      "======================================================================\n",
      "Polynomial feature matrix shape: (1000, 5)\n",
      "Features: X^1 X^2 X^3 X^4 X^5\n",
      "With intercept: (1000, 6)\n",
      "\n",
      "Coefficients:\n",
      "  θ₀ (intercept) = 473.5186\n",
      "  θ_1 (X^1)       = 170.8052\n",
      "  θ_2 (X^2)       = 38.6424\n",
      "  θ_3 (X^3)       = 12.3476\n",
      "  ... (and 2 more coefficients)\n",
      "\n",
      "Performance Metrics:\n",
      "  Training - MSE:    52,626.11  |  R²:  0.5429\n",
      "  Testing  - MSE:   570,616.91  |  R²: -2.4225\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      " Degree    Train_MSE  Train_R2      Test_MSE   Test_R2\n",
      "      1 57947.526161  0.496709  88575.978543  0.468736\n",
      "      2 54822.665116  0.523849  71791.679479  0.569406\n",
      "      3 53785.194716  0.532860  99833.483763  0.401216\n",
      "      4 52795.774758  0.541453 250979.274285 -0.505331\n",
      "      5 52626.111955  0.542927 570616.914821 -2.422464\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Best Training Performance: Degree 5 (MSE: 52626.11)\n",
      "Best Testing Performance:  Degree 2 (MSE: 71791.68)\n",
      "\n",
      "⚠ OVERFITTING DETECTED:\n",
      "   Degree 5 fits training data best\n",
      "   But degree 2 generalizes better to test data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../house_data/train.csv')\n",
    "test_df = pd.read_csv('../house_data/test.csv')\n",
    "\n",
    "# Get target variable\n",
    "y_train = train_df['price'].values / 1000\n",
    "y_test = test_df['price'].values / 1000\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROBLEM 4: POLYNOMIAL REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def train_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Train linear regression using closed-form solution\n",
    "    theta = (X^T X)^(-1) X^T y\n",
    "    \"\"\"\n",
    "    XTX = X.T @ X\n",
    "    theta = np.linalg.inv(XTX) @ X.T @ y\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"Predict response for new data points\"\"\"\n",
    "    return X @ theta\n",
    "\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared score\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "\n",
    "def create_polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    Create polynomial features up to given degree\n",
    "    \n",
    "    Example: If X = [2, 3] and degree = 3\n",
    "    Returns: [[2, 4, 8],    # [2^1, 2^2, 2^3]\n",
    "              [3, 9, 27]]   # [3^1, 3^2, 3^3]\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array (n_samples,)\n",
    "        Single feature values\n",
    "    degree : int\n",
    "        Maximum degree of polynomial\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_poly : array (n_samples, degree)\n",
    "        Polynomial features [X, X^2, X^3, ..., X^degree]\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    X_poly = np.zeros((n_samples, degree))\n",
    "    \n",
    "    # Create features: X, X^2, X^3, ..., X^p\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly[:, d-1] = X ** d\n",
    "    \n",
    "    return X_poly\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: EXTRACT AND PREPARE DATA\n",
    "# ============================================\n",
    "print(\"\\nSTEP 1: Extract sqft_living feature\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "X_train_sqft = train_df['sqft_living'].values\n",
    "X_test_sqft = test_df['sqft_living'].values\n",
    "\n",
    "print(f\"Train samples: {len(X_train_sqft)}\")\n",
    "print(f\"Test samples:  {len(X_test_sqft)}\")\n",
    "print(f\"sqft_living range: [{X_train_sqft.min():.0f}, {X_train_sqft.max():.0f}]\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: STANDARDIZE FEATURE\n",
    "# ============================================\n",
    "print(\"\\nSTEP 2: Standardize sqft_living\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "mean_sqft = X_train_sqft.mean()\n",
    "std_sqft = X_train_sqft.std(ddof=1)\n",
    "\n",
    "X_train_sqft_scaled = (X_train_sqft - mean_sqft) / std_sqft\n",
    "X_test_sqft_scaled = (X_test_sqft - mean_sqft) / std_sqft\n",
    "\n",
    "print(f\"Original - Mean: {mean_sqft:.2f}, Std: {std_sqft:.2f}\")\n",
    "print(f\"Scaled   - Mean: {X_train_sqft_scaled.mean():.6f}, Std: {X_train_sqft_scaled.std(ddof=1):.6f}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: TRAIN POLYNOMIAL MODELS\n",
    "# ============================================\n",
    "print(\"\\nSTEP 3: Train polynomial models for degrees 1-5\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "results = []\n",
    "\n",
    "for p in degrees:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"DEGREE p = {p}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    # Create polynomial features: [X, X^2, X^3, ..., X^p]\n",
    "    X_train_poly = create_polynomial_features(X_train_sqft_scaled, p)\n",
    "    X_test_poly = create_polynomial_features(X_test_sqft_scaled, p)\n",
    "    \n",
    "    print(f\"Polynomial feature matrix shape: {X_train_poly.shape}\")\n",
    "    print(f\"Features: \", end=\"\")\n",
    "    for d in range(1, p+1):\n",
    "        print(f\"X^{d}\", end=\" \" if d < p else \"\\n\")\n",
    "    \n",
    "    # Add intercept column: [1, X, X^2, X^3, ..., X^p]\n",
    "    X_train_poly_int = np.c_[np.ones(X_train_poly.shape[0]), X_train_poly]\n",
    "    X_test_poly_int = np.c_[np.ones(X_test_poly.shape[0]), X_test_poly]\n",
    "    \n",
    "    print(f\"With intercept: {X_train_poly_int.shape}\")\n",
    "    \n",
    "    # Train model using closed-form solution\n",
    "    theta_poly = train_linear_regression(X_train_poly_int, y_train)\n",
    "    \n",
    "    # Display coefficients\n",
    "    print(f\"\\nCoefficients:\")\n",
    "    print(f\"  theta_0 (intercept) = {theta_poly[0]:.4f}\")\n",
    "    for d in range(1, min(p+1, 4)):  # Show first 3 coefficients\n",
    "        print(f\"  theta_{d} (X^{d})       = {theta_poly[d]:.4f}\")\n",
    "    if p > 3:\n",
    "        print(f\"  ... (and {p-3} more coefficients)\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred_poly = predict(X_train_poly_int, theta_poly)\n",
    "    y_test_pred_poly = predict(X_test_poly_int, theta_poly)\n",
    "    \n",
    "    # Compute metrics\n",
    "    train_mse_poly = compute_mse(y_train, y_train_pred_poly)\n",
    "    train_r2_poly = compute_r2(y_train, y_train_pred_poly)\n",
    "    test_mse_poly = compute_mse(y_test, y_test_pred_poly)\n",
    "    test_r2_poly = compute_r2(y_test, y_test_pred_poly)\n",
    "    \n",
    "    print(f\"\\nPerformance Metrics:\")\n",
    "    print(f\"  Training - MSE: {train_mse_poly:>12,.2f}  |  R2: {train_r2_poly:>7.4f}\")\n",
    "    print(f\"  Testing  - MSE: {test_mse_poly:>12,.2f}  |  R2: {test_r2_poly:>7.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Degree': p,\n",
    "        'Train_MSE': train_mse_poly,\n",
    "        'Train_R2': train_r2_poly,\n",
    "        'Test_MSE': test_mse_poly,\n",
    "        'Test_R2': test_r2_poly\n",
    "    })\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: SUMMARY AND ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Find best models\n",
    "best_train_idx = results_df['Train_MSE'].idxmin()\n",
    "best_test_idx = results_df['Test_MSE'].idxmin()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ANALYSIS\")\n",
    "print('='*70)\n",
    "print(f\"\\nBest Training Performance: Degree {results_df.loc[best_train_idx, 'Degree']} \"\n",
    "      f\"(MSE: {results_df.loc[best_train_idx, 'Train_MSE']:.2f})\")\n",
    "print(f\"Best Testing Performance:  Degree {results_df.loc[best_test_idx, 'Degree']} \"\n",
    "      f\"(MSE: {results_df.loc[best_test_idx, 'Test_MSE']:.2f})\")\n",
    "\n",
    "# Check for overfitting\n",
    "if best_train_idx != best_test_idx:\n",
    "    print(f\"\\n[!] OVERFITTING DETECTED:\")\n",
    "    print(f\"   Degree {results_df.loc[best_train_idx, 'Degree']} fits training data best\")\n",
    "    print(f\"   But degree {results_df.loc[best_test_idx, 'Degree']} generalizes better to test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405607e5",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Training Performance: As polynomial degree increases from 1 to 5, training MSE consistently decreases (57,948 -> 52,626) and R^2 increases (0.497 -> 0.543). This indicates the model is fitting the training data progressively better with more complex polynomials.\n",
    "Testing Performance: Test MSE is lowest at degree 2 (71,792) with R^2 = 0.569. Beyond degree 2, test MSE increases dramatically, reaching 570,617 at degree 5, with negative R^2 values at degrees 4 and 5, indicating performance worse than a horizontal line.\n",
    "Overfitting: Clear overfitting occurs at degrees 3-5. While these models fit training data better, they perform catastrophically on test data. The negative R^2 values mean the model is making predictions worse than simply predicting the mean price. High-degree polynomials capture noise and random fluctuations in training data rather than true underlying patterns.\n",
    "Optimal Degree: Degree 2 (quadratic) provides the best balance, achieving the lowest test MSE and highest test R^2 (0.569). This suggests the relationship between square footage and price follows a quadratic curve rather than a linear or higher-order relationship.\n",
    "\n",
    "Conclusion:\n",
    "The optimal polynomial degree is p = 2. This demonstrates the bias-variance tradeoff: low degrees (p=1) underfit with high bias, while high degrees (p>=3) overfit with high variance. Degree 2 strikes the right balance for generalization to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
