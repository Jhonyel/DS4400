{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7550036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM 5: GRADIENT DESCENT FOR LINEAR REGRESSION\n",
      "======================================================================\n",
      "\n",
      "Dataset:\n",
      "  Features: 18\n",
      "  Training samples: 1000\n",
      "  Testing samples: 1000\n",
      "\n",
      "Standardization check:\n",
      "  Train mean: -0.000000 (should be ~0)\n",
      "  Train std:  0.999500 (should be ~1)\n",
      "  Final shape with intercept: (1000, 19)\n",
      "\n",
      "======================================================================\n",
      "BASELINE: Results from Problem 3 (sklearn/closed-form)\n",
      "======================================================================\n",
      "\n",
      "Optimal (Target) Solution from Problem 3:\n",
      "  Training - MSE:    31,415.75  |  R2:  0.7271\n",
      "  Testing  - MSE:    58,834.67  |  R2:  0.6471\n",
      "\n",
      "[OK] Using Problem 3 results as ground truth for comparison\n",
      "\n",
      "======================================================================\n",
      "GRADIENT DESCENT EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LEARNING RATE: alpha = 0.01\n",
      "======================================================================\n",
      "\n",
      "  [10 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:   235,755.43  |  R2: -1.0476\n",
      "    Test MSE:    282,905.92  |  R2: -0.6968\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 787.2113\n",
      "    MSE difference: 650.44%\n",
      "    Status: [NOT CONVERGED]\n",
      "\n",
      "  [50 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 168,289.31\n",
      "    Iteration  40: Cost = 91,170.06\n",
      "    Iteration  50: Cost = 71,274.06\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:    69,700.36  |  R2:  0.3946\n",
      "    Test MSE:     94,333.51  |  R2:  0.4342\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 671.5389\n",
      "    MSE difference: 121.86%\n",
      "    Status: [NOT CONVERGED]\n",
      "\n",
      "  [100 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 168,289.31\n",
      "    Iteration  40: Cost = 91,170.06\n",
      "    Iteration  60: Cost = 58,098.81\n",
      "    Iteration  80: Cost = 43,495.42\n",
      "    Iteration 100: Cost = 36,974.76\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:    36,766.16  |  R2:  0.6807\n",
      "    Test MSE:     61,282.47  |  R2:  0.6324\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 638.6699\n",
      "    MSE difference: 17.03%\n",
      "    Status: [PARTIAL]\n",
      "\n",
      "======================================================================\n",
      "LEARNING RATE: alpha = 0.1\n",
      "======================================================================\n",
      "\n",
      "  [10 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:    35,048.98  |  R2:  0.6956\n",
      "    Test MSE:     60,006.49  |  R2:  0.6401\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 636.5196\n",
      "    MSE difference: 11.56%\n",
      "    Status: [PARTIAL]\n",
      "\n",
      "  [50 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 31,622.94\n",
      "    Iteration  40: Cost = 31,442.56\n",
      "    Iteration  50: Cost = 31,428.02\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:    31,427.11  |  R2:  0.7270\n",
      "    Test MSE:     58,890.70  |  R2:  0.6468\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 629.6314\n",
      "    MSE difference: 0.04%\n",
      "    Status: [CONVERGED]\n",
      "\n",
      "  [100 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 31,622.94\n",
      "    Iteration  40: Cost = 31,442.56\n",
      "    Iteration  60: Cost = 31,421.45\n",
      "    Iteration  80: Cost = 31,417.02\n",
      "    Iteration 100: Cost = 31,416.04\n",
      "\n",
      "  Performance:\n",
      "    Train MSE:    31,416.02  |  R2:  0.7271\n",
      "    Test MSE:     58,839.95  |  R2:  0.6471\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 630.3572\n",
      "    MSE difference: 0.00%\n",
      "    Status: [CONVERGED]\n",
      "\n",
      "======================================================================\n",
      "LEARNING RATE: alpha = 0.5\n",
      "======================================================================\n",
      "\n",
      "  [10 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "\n",
      "  Performance:\n",
      "    Train MSE: 142,861,187,964,522,576.00  |  R2: -1240791164860.3005\n",
      "    Test MSE:  159,252,038,043,262,112.00  |  R2: -955166754273.0941\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 165667298.3233\n",
      "    MSE difference: 454743840158172.75%\n",
      "    Status: [NOT CONVERGED]\n",
      "\n",
      "  [50 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 24,162,051,872,495,995,797,758,803,968.00\n",
      "    Iteration  40: Cost = 216,138,843,756,041,429,542,048,059,740,742,759,133,636,295,036,764,160.00\n",
      "    Iteration  50: Cost = 646,446,114,744,139,966,872,027,668,260,357,978,867,112,519,886,364,298,703,756,328,960.00\n",
      "\n",
      "  Performance:\n",
      "    Train MSE: 11,431,738,466,768,859,532,549,956,230,600,599,748,510,636,921,103,938,850,776,344,952,832.00  |  R2: -99287989205958109269870581624577748265479471149765164682706944.0000\n",
      "    Test MSE:  12,743,331,132,033,843,844,151,670,850,176,625,799,379,529,345,418,836,460,354,590,998,528.00  |  R2: -76432342000660357926079014846588307371352150634919347158515712.0000\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 1481958658930172584614286099218432.0000\n",
      "    MSE difference: 36388558181067969266315976684711766731324117094708654477202685952.00%\n",
      "    Status: [NOT CONVERGED]\n",
      "\n",
      "  [100 iterations]\n",
      "  ------------------------------------------------------------------\n",
      "    Iteration   1: Cost = 385,968.77\n",
      "    Iteration  20: Cost = 24,162,051,872,495,995,797,758,803,968.00\n",
      "    Iteration  40: Cost = 216,138,843,756,041,429,542,048,059,740,742,759,133,636,295,036,764,160.00\n",
      "    Iteration  60: Cost = 1,933,445,057,841,959,268,939,252,762,520,145,803,197,052,894,539,965,638,864,143,290,995,818,035,675,136.00\n",
      "    Iteration  80: Cost = 17,295,409,407,819,664,574,924,662,765,453,490,246,385,447,351,203,137,262,706,558,229,003,324,630,225,113,245,746,126,930,777,743,556,608.00\n",
      "    Iteration 100: Cost = 154,714,086,842,465,628,029,973,935,994,655,704,609,674,739,323,045,464,950,077,562,833,173,969,060,519,173,065,065,451,905,575,447,646,964,975,188,524,555,448,606,523,392.00\n",
      "\n",
      "  Performance:\n",
      "    Train MSE: 2,735,960,411,190,739,557,046,500,675,017,799,549,336,846,413,528,979,064,236,923,296,082,795,419,138,854,712,626,187,131,280,563,780,174,385,081,858,951,435,757,111,214,080.00  |  R2: -23762615682985900960314771811058871302785878703441693968691921853049006636693906819574251828586159436025879516339493910085632.0000\n",
      "    Test MSE:  3,049,864,164,167,988,868,489,223,852,634,374,698,584,491,952,495,137,438,802,071,059,632,339,473,971,724,636,362,471,318,463,210,726,060,235,007,967,784,665,489,411,670,016.00  |  R2: -18292568751137963385693669050323505485752476087375039643801495486788641893149204262671888360017586319422082940402547661209600.0000\n",
      "\n",
      "  Convergence:\n",
      "    Distance from optimal: 22926346559359337055268887033424971475271615286420829006790131712.0000\n",
      "    MSE difference: 8708881408817995659373539442491620826501993751536738419397469158803708133655439521122230043500667554930604814180336720888725504.00%\n",
      "    Status: [NOT CONVERGED]\n",
      "\n",
      "======================================================================\n",
      "SUMMARY TABLE\n",
      "======================================================================\n",
      "\n",
      " Alpha  Iterations     Train_MSE       Train_R2      Test_MSE        Test_R2        Status\n",
      "  0.01          10  2.357554e+05  -1.047605e+00  2.829059e+05  -6.968218e-01 NOT CONVERGED\n",
      "  0.01          50  6.970036e+04   3.946320e-01  9.433351e+04   4.342036e-01 NOT CONVERGED\n",
      "  0.01         100  3.676616e+04   6.806752e-01  6.128247e+04   6.324381e-01       PARTIAL\n",
      "  0.10          10  3.504898e+04   6.955894e-01  6.000649e+04   6.400912e-01       PARTIAL\n",
      "  0.10          50  3.142711e+04   7.270464e-01  5.889070e+04   6.467836e-01     CONVERGED\n",
      "  0.10         100  3.141602e+04   7.271427e-01  5.883995e+04   6.470879e-01     CONVERGED\n",
      "  0.50          10  1.428612e+17  -1.240791e+12  1.592520e+17  -9.551668e+11 NOT CONVERGED\n",
      "  0.50          50  1.143174e+67  -9.928799e+61  1.274333e+67  -7.643234e+61 NOT CONVERGED\n",
      "  0.50         100 2.735960e+129 -2.376262e+124 3.049864e+129 -1.829257e+124 NOT CONVERGED\n",
      "\n",
      "======================================================================\n",
      "BASELINE (Target from Closed-Form):\n",
      "  Train MSE:  31,415.75  |  Train R2: 0.7271\n",
      "  Test MSE:   58,834.67  |  Test R2:  0.6471\n",
      "\n",
      "======================================================================\n",
      "ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "1. LEARNING RATE EFFECTS:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   alpha = 0.01:\n",
      "     Best at 100 iterations\n",
      "     MSE difference: 17.03%\n",
      "     Status: PARTIAL\n",
      "\n",
      "   alpha = 0.1:\n",
      "     Best at 100 iterations\n",
      "     MSE difference: 0.00%\n",
      "     Status: CONVERGED\n",
      "\n",
      "   alpha = 0.5:\n",
      "     Best at 10 iterations\n",
      "     MSE difference: 454743840158172.75%\n",
      "     Status: NOT CONVERGED\n",
      "\n",
      "2. CONVERGENCE PROGRESSION:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   alpha = 0.01:\n",
      "      10 iter: MSE diff = 650.44% [NOT CONVERGED]\n",
      "      50 iter: MSE diff = 121.86% [NOT CONVERGED]\n",
      "     100 iter: MSE diff =  17.03% [PARTIAL]\n",
      "\n",
      "   alpha = 0.1:\n",
      "      10 iter: MSE diff =  11.56% [PARTIAL]\n",
      "      50 iter: MSE diff =   0.04% [CONVERGED]\n",
      "     100 iter: MSE diff =   0.00% [CONVERGED]\n",
      "\n",
      "   alpha = 0.5:\n",
      "      10 iter: MSE diff = 454743840158172.75% [NOT CONVERGED]\n",
      "      50 iter: MSE diff = 36388558181067969266315976684711766731324117094708654477202685952.00% [NOT CONVERGED]\n",
      "     100 iter: MSE diff = 8708881408817995659373539442491620826501993751536738419397469158803708133655439521122230043500667554930604814180336720888725504.00% [NOT CONVERGED]\n",
      "\n",
      "3. BEST OVERALL CONFIGURATION:\n",
      "   ------------------------------------------------------------------\n",
      "\n",
      "   Learning rate: 0.1\n",
      "   Iterations: 100\n",
      "   Train MSE: 31,416.02\n",
      "   Test MSE: 58,839.95\n",
      "   Test R2: 0.6471\n",
      "   Difference from optimal: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('../house_data/train.csv')\n",
    "test_df = pd.read_csv('../house_data/test.csv')\n",
    "\n",
    "# Remove excluded columns\n",
    "potential_excludes = ['id', 'date', 'zipcode']\n",
    "cols_to_drop = [col for col in potential_excludes if col in train_df.columns]\n",
    "\n",
    "# Separate features and target\n",
    "X_train = train_df.drop(columns=cols_to_drop + ['price'])\n",
    "y_train = train_df['price'].values / 1000\n",
    "\n",
    "X_test = test_df.drop(columns=cols_to_drop + ['price'])\n",
    "y_test = test_df['price'].values / 1000\n",
    "\n",
    "# Ensure same columns\n",
    "common_cols = X_train.columns.tolist()\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PROBLEM 5: GRADIENT DESCENT FOR LINEAR REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Features: {len(common_cols)}\")\n",
    "print(f\"  Training samples: {len(y_train)}\")\n",
    "print(f\"  Testing samples: {len(y_test)}\")\n",
    "\n",
    "# Standardize features (CRITICAL: Use same method as Problem 3)\n",
    "X_train_np = X_train.values\n",
    "X_test_np = X_test.values\n",
    "\n",
    "mean = X_train_np.mean(axis=0)\n",
    "std = X_train_np.std(axis=0, ddof=1)\n",
    "std[std == 0] = 1.0\n",
    "\n",
    "X_train_scaled = (X_train_np - mean) / std\n",
    "X_test_scaled = (X_test_np - mean) / std\n",
    "\n",
    "# Verify standardization\n",
    "print(f\"\\nStandardization check:\")\n",
    "print(f\"  Train mean: {X_train_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"  Train std:  {X_train_scaled.std():.6f} (should be ~1)\")\n",
    "\n",
    "# Add intercept\n",
    "X_train_with_int = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "X_test_with_int = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n",
    "\n",
    "print(f\"  Final shape with intercept: {X_train_with_int.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\"Make predictions using current theta\"\"\"\n",
    "    return X @ theta\n",
    "\n",
    "\n",
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"Compute Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def compute_r2(y_true, y_pred):\n",
    "    \"\"\"Compute R-squared score\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, alpha, num_iterations, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for linear regression\n",
    "    \n",
    "    Cost function: J(theta) = (1/n) * sum((X@theta - y)^2)\n",
    "    Gradient: dJ/d(theta) = (2/n) * X^T @ (X@theta - y)\n",
    "    Update rule: theta = theta - alpha * gradient\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array (n_samples, n_features)\n",
    "        Feature matrix with intercept\n",
    "    y : array (n_samples,)\n",
    "        Target values\n",
    "    alpha : float\n",
    "        Learning rate\n",
    "    num_iterations : int\n",
    "        Number of gradient descent iterations\n",
    "    verbose : bool\n",
    "        If True, print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    theta : array (n_features,)\n",
    "        Optimized parameters\n",
    "    cost_history : list\n",
    "        Cost at each iteration\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize theta to zeros\n",
    "    theta = np.zeros(n_features)\n",
    "    \n",
    "    # Track cost history\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # 1. Compute predictions\n",
    "        predictions = X @ theta\n",
    "        \n",
    "        # 2. Compute errors\n",
    "        errors = predictions - y\n",
    "        \n",
    "        # 3. Compute gradient: (2/n) * X^T @ errors\n",
    "        gradient = (2 / n_samples) * X.T @ errors\n",
    "        \n",
    "        # 4. Update theta\n",
    "        theta = theta - alpha * gradient\n",
    "        \n",
    "        # 5. Track cost\n",
    "        cost = np.mean(errors ** 2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Print progress for verbose mode\n",
    "        if verbose and (i == 0 or (i + 1) % 20 == 0 or i == num_iterations - 1):\n",
    "            print(f\"    Iteration {i+1:3d}: Cost = {cost:,.2f}\")\n",
    "    \n",
    "    return theta, cost_history\n",
    "\n",
    "\n",
    "def train_closed_form(X, y):\n",
    "    \"\"\"Train using closed-form solution for comparison\"\"\"\n",
    "    theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return theta\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE: Results from Problem 3 (sklearn/closed-form)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_mse_opt = 31415.75  # From Problem 3 sklearn\n",
    "train_r2_opt = 0.7271     # From Problem 3 sklearn\n",
    "test_mse_opt = 58834.67   # From Problem 3 sklearn  \n",
    "test_r2_opt = 0.6471      # From Problem 3 sklearn\n",
    "\n",
    "print(f\"\\nOptimal (Target) Solution from Problem 3:\")\n",
    "print(f\"  Training - MSE: {train_mse_opt:>12,.2f}  |  R2: {train_r2_opt:>7.4f}\")\n",
    "print(f\"  Testing  - MSE: {test_mse_opt:>12,.2f}  |  R2: {test_r2_opt:>7.4f}\")\n",
    "print(\"\\n[OK] Using Problem 3 results as ground truth for comparison\")\n",
    "\n",
    "theta_optimal = train_closed_form(X_train_with_int, y_train)\n",
    "\n",
    "# ============================================\n",
    "# GRADIENT DESCENT EXPERIMENTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GRADIENT DESCENT EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "iterations_list = [10, 50, 100]\n",
    "\n",
    "results = []\n",
    "\n",
    "for alpha in learning_rates:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LEARNING RATE: alpha = {alpha}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    for num_iter in iterations_list:\n",
    "        print(f\"\\n  [{num_iter} iterations]\")\n",
    "        print(f\"  {'-'*66}\")\n",
    "        \n",
    "        # Train with gradient descent\n",
    "        theta_gd, cost_history = gradient_descent(\n",
    "            X_train_with_int, \n",
    "            y_train, \n",
    "            alpha=alpha, \n",
    "            num_iterations=num_iter,\n",
    "            verbose=(num_iter >= 50)\n",
    "        )\n",
    "        \n",
    "        # Check for divergence\n",
    "        if np.isnan(theta_gd).any() or np.isinf(theta_gd).any():\n",
    "            print(f\"  [DIVERGED] Learning rate {alpha} is too large!\")\n",
    "            results.append({\n",
    "                'Alpha': alpha,\n",
    "                'Iterations': num_iter,\n",
    "                'Train_MSE': np.nan,\n",
    "                'Train_R2': np.nan,\n",
    "                'Test_MSE': np.nan,\n",
    "                'Test_R2': np.nan,\n",
    "                'Theta_Distance': np.nan,\n",
    "                'Status': 'DIVERGED'\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Make predictions\n",
    "        y_train_pred_gd = predict(X_train_with_int, theta_gd)\n",
    "        y_test_pred_gd = predict(X_test_with_int, theta_gd)\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_mse_gd = compute_mse(y_train, y_train_pred_gd)\n",
    "        train_r2_gd = compute_r2(y_train, y_train_pred_gd)\n",
    "        test_mse_gd = compute_mse(y_test, y_test_pred_gd)\n",
    "        test_r2_gd = compute_r2(y_test, y_test_pred_gd)\n",
    "        \n",
    "        # Compare with optimal\n",
    "        theta_diff = np.linalg.norm(theta_gd - theta_optimal)\n",
    "        mse_diff_pct = abs(train_mse_gd - train_mse_opt) / train_mse_opt * 100\n",
    "        \n",
    "        # Determine status\n",
    "        if mse_diff_pct < 0.1:\n",
    "            status = \"CONVERGED\"\n",
    "        elif mse_diff_pct < 1:\n",
    "            status = \"VERY CLOSE\"\n",
    "        elif mse_diff_pct < 5:\n",
    "            status = \"CLOSE\"\n",
    "        elif mse_diff_pct < 20:\n",
    "            status = \"PARTIAL\"\n",
    "        else:\n",
    "            status = \"NOT CONVERGED\"\n",
    "        \n",
    "        print(f\"\\n  Performance:\")\n",
    "        print(f\"    Train MSE: {train_mse_gd:>12,.2f}  |  R2: {train_r2_gd:>7.4f}\")\n",
    "        print(f\"    Test MSE:  {test_mse_gd:>12,.2f}  |  R2: {test_r2_gd:>7.4f}\")\n",
    "        print(f\"\\n  Convergence:\")\n",
    "        print(f\"    Distance from optimal: {theta_diff:.4f}\")\n",
    "        print(f\"    MSE difference: {mse_diff_pct:.2f}%\")\n",
    "        print(f\"    Status: [{status}]\")\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Alpha': alpha,\n",
    "            'Iterations': num_iter,\n",
    "            'Train_MSE': train_mse_gd,\n",
    "            'Train_R2': train_r2_gd,\n",
    "            'Test_MSE': test_mse_gd,\n",
    "            'Test_R2': test_r2_gd,\n",
    "            'Theta_Distance': theta_diff,\n",
    "            'MSE_Diff_Pct': mse_diff_pct,\n",
    "            'Status': status\n",
    "        })\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY TABLE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display clean table\n",
    "summary_cols = ['Alpha', 'Iterations', 'Train_MSE', 'Train_R2', 'Test_MSE', 'Test_R2', 'Status']\n",
    "print(\"\\n\" + results_df[summary_cols].to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BASELINE (Target from Closed-Form):\")\n",
    "print(f\"  Train MSE: {train_mse_opt:>10,.2f}  |  Train R2: {train_r2_opt:>6.4f}\")\n",
    "print(f\"  Test MSE:  {test_mse_opt:>10,.2f}  |  Test R2:  {test_r2_opt:>6.4f}\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter out diverged results\n",
    "converged_df = results_df[results_df['Status'] != 'DIVERGED']\n",
    "\n",
    "if len(converged_df) > 0:\n",
    "    print(\"\\n1. LEARNING RATE EFFECTS:\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    \n",
    "    for alpha in learning_rates:\n",
    "        alpha_results = converged_df[converged_df['Alpha'] == alpha]\n",
    "        if len(alpha_results) == 0:\n",
    "            print(f\"\\n   alpha = {alpha}: [DIVERGED - too large!]\")\n",
    "            continue\n",
    "        \n",
    "        best_idx = alpha_results['MSE_Diff_Pct'].idxmin()\n",
    "        best = alpha_results.loc[best_idx]\n",
    "        \n",
    "        print(f\"\\n   alpha = {alpha}:\")\n",
    "        print(f\"     Best at {int(best['Iterations'])} iterations\")\n",
    "        print(f\"     MSE difference: {best['MSE_Diff_Pct']:.2f}%\")\n",
    "        print(f\"     Status: {best['Status']}\")\n",
    "    \n",
    "    print(\"\\n2. CONVERGENCE PROGRESSION:\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    \n",
    "    for alpha in learning_rates:\n",
    "        alpha_results = converged_df[converged_df['Alpha'] == alpha].sort_values('Iterations')\n",
    "        if len(alpha_results) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n   alpha = {alpha}:\")\n",
    "        for _, row in alpha_results.iterrows():\n",
    "            print(f\"     {int(row['Iterations']):3d} iter: MSE diff = {row['MSE_Diff_Pct']:6.2f}% [{row['Status']}]\")\n",
    "    \n",
    "    print(\"\\n3. BEST OVERALL CONFIGURATION:\")\n",
    "    print(\"   \" + \"-\"*66)\n",
    "    \n",
    "    best_overall = converged_df.loc[converged_df['MSE_Diff_Pct'].idxmin()]\n",
    "    print(f\"\\n   Learning rate: {best_overall['Alpha']}\")\n",
    "    print(f\"   Iterations: {int(best_overall['Iterations'])}\")\n",
    "    print(f\"   Train MSE: {best_overall['Train_MSE']:,.2f}\")\n",
    "    print(f\"   Test MSE: {best_overall['Test_MSE']:,.2f}\")\n",
    "    print(f\"   Test R2: {best_overall['Test_R2']:.4f}\")\n",
    "    print(f\"   Difference from optimal: {best_overall['MSE_Diff_Pct']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6008f",
   "metadata": {},
   "source": [
    "## Problem 5: Gradient Descent Observations\n",
    "\n",
    "### Learning Rate Effects:\n",
    "\n",
    "- **alpha = 0.01 (Too Small):** Converges slowly. After 100 iterations, MSE = 36,766 (17% from optimal). Stable but inefficient.\n",
    "\n",
    "- **alpha = 0.1 (Optimal):** Best performance. Reaches optimal solution (MSE = 31,416, 0.00% difference) in 100 iterations. Within 0.04% at 50 iterations.\n",
    "\n",
    "- **alpha = 0.5 (Too Large):** Diverges completely. Cost explodes to astronomical values (10^129+). Demonstrates failure when learning rate is too large.\n",
    "\n",
    "### Iteration Requirements:\n",
    "\n",
    "- **10 iterations:** Insufficient. alpha = 0.1 is 11.56% from optimal.\n",
    "- **50 iterations:** alpha = 0.1 converges (0.04% difference). alpha = 0.01 still needs more (122% off).\n",
    "- **100 iterations:** alpha = 0.1 fully converges (0.00% difference). alpha = 0.01 improves to 17.03% but not converged.\n",
    "\n",
    "### Convergence to Optimal:\n",
    "\n",
    "- **Yes, gradient descent converges to the closed-form solution** with proper configuration.\n",
    "- **alpha = 0.1, 100 iterations** matches sklearn exactly: Train MSE = 31,416, R^2 = 0.727; Test MSE = 58,840, R^2 = 0.647.\n",
    "- Learning rate is critical: too small = slow; too large = divergence; balanced = optimal.\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- Feature standardization is essential for gradient descent effectiveness\n",
    "- Demonstrates bias-variance-speed tradeoff in iterative optimization\n",
    "- With proper hyperparameters, iterative methods match analytical solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
